Principle component analysis is technique for finding the most important combination of features of a data set. In addition to being widely used in machine learning, it's also used to visualize high dimensional data. Let's walk through a toy example of reducing a 2d data set to a 1d data set.

Notice below how the data points vary the most along the blue line. This blue line is what's call the '1st principle component' (or PC1) of our data set. Drag the points around to see how PC1 changes.

[Visualization of PC with only two dimensions]

The red line is the the 2nd principle component (or PC2.) It's the line along which the data varies the second most.

Another way to look at this data is as the image of the data projected onto PC1. This reduces the dimension of our data set from 2 to 1. Depending on our data set, we might only loose a little bit of information. This is the case if our data is highly correlated.

We can easily perceive two dimensions fairly well in a simple plot so this example is somewhat contrived but imagine we had 100 or 1,000 dimensions.

For some data sets, particularly high dimensional data sets (aka., data sets with many variables) many of the variables will be redundant (aka., highly correlated.)